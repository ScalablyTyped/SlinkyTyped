package typingsSlinky.tensorflowTfjsConverter

import typingsSlinky.tensorflowTfjsConverter.typesMod.NamedTensorsMap
import typingsSlinky.tensorflowTfjsConverter.typesMod.TensorInfo
import typingsSlinky.tensorflowTfjsCore.distTypesMod.Rank
import typingsSlinky.tensorflowTfjsCore.modelTypesMod.InferenceModel
import typingsSlinky.tensorflowTfjsCore.tensorMod.Tensor
import typingsSlinky.tensorflowTfjsCore.tensorTypesMod.NamedTensorMap
import typingsSlinky.tensorflowTfjsCore.typesMod.IOHandler
import typingsSlinky.tensorflowTfjsCore.typesMod.LoadOptions
import typingsSlinky.tensorflowTfjsCore.typesMod.ModelArtifacts
import typingsSlinky.tensorflowTfjsCore.typesMod.SaveConfig
import typingsSlinky.tensorflowTfjsCore.typesMod.SaveResult
import scala.scalajs.js
import scala.scalajs.js.`|`
import scala.scalajs.js.annotation.{JSGlobalScope, JSGlobal, JSImport, JSName, JSBracketAccess}

@JSImport("@tensorflow/tfjs-converter/dist/executor/graph_model", JSImport.Namespace)
@js.native
object graphModelMod extends js.Object {
  
  val DEFAULT_MODEL_NAME: /* "model.json" */ String = js.native
  
  val TFHUB_SEARCH_PARAM: /* "?tfjs-format=file" */ String = js.native
  
  def loadGraphModel(modelUrl: String): js.Promise[GraphModel] = js.native
  def loadGraphModel(modelUrl: String, options: LoadOptions): js.Promise[GraphModel] = js.native
  def loadGraphModel(modelUrl: IOHandler): js.Promise[GraphModel] = js.native
  def loadGraphModel(modelUrl: IOHandler, options: LoadOptions): js.Promise[GraphModel] = js.native
  
  @js.native
  class GraphModel protected () extends InferenceModel {
    /**
      * @param modelUrl url for the model, or an `io.IOHandler`.
      * @param weightManifestUrl url for the weight file generated by
      * scripts/convert.py script.
      * @param requestOption options for Request, which allows to send credentials
      * and custom headers.
      * @param onProgress Optional, progress callback function, fired periodically
      * before the load is completed.
      */
    def this(modelUrl: String) = this()
    def this(modelUrl: IOHandler) = this()
    def this(modelUrl: String, loadOptions: LoadOptions) = this()
    def this(modelUrl: IOHandler, loadOptions: LoadOptions) = this()
    
    var artifacts: js.Any = js.native
    
    var convertTensorMapToTensorsMap: js.Any = js.native
    
    /**
      * Releases the memory used by the weight tensors.
      */
    /** @doc {heading: 'Models', subheading: 'Classes'} */
    def dispose(): Unit = js.native
    
    def execute(inputs: js.Array[Tensor[Rank]]): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
    /**
      * Executes inference for the model for given input tensors.
      * @param inputs tensor, tensor array or tensor map of the inputs for the
      * model, keyed by the input node names.
      * @param outputs output node name from the Tensorflow model, if no
      * outputs are specified, the default outputs of the model would be used.
      * You can inspect intermediate nodes of the model by adding them to the
      * outputs array.
      *
      * @returns A single tensor if provided with a single output or no outputs
      * are provided and there is only one default output, otherwise return a
      * tensor array. The order of the tensor array is the same as the outputs
      * if provided, otherwise the order of outputNodes attribute of the model.
      */
    /** @doc {heading: 'Models', subheading: 'Classes'} */
    def execute(inputs: Tensor[Rank]): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
    def execute(inputs: NamedTensorMap): Tensor[Rank] | js.Array[Tensor[Rank]] = js.native
    
    def executeAsync(inputs: js.Array[Tensor[Rank]]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: js.Array[Tensor[Rank]], outputs: String): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: js.Array[Tensor[Rank]], outputs: js.Array[String]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    /**
      * Executes inference for the model for given input tensors in async
      * fashion, use this method when your model contains control flow ops.
      * @param inputs tensor, tensor array or tensor map of the inputs for the
      * model, keyed by the input node names.
      * @param outputs output node name from the Tensorflow model, if no outputs
      * are specified, the default outputs of the model would be used. You can
      * inspect intermediate nodes of the model by adding them to the outputs
      * array.
      *
      * @returns A Promise of single tensor if provided with a single output or
      * no outputs are provided and there is only one default output, otherwise
      * return a tensor map.
      */
    /** @doc {heading: 'Models', subheading: 'Classes'} */
    def executeAsync(inputs: Tensor[Rank]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: Tensor[Rank], outputs: String): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: Tensor[Rank], outputs: js.Array[String]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: NamedTensorMap): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: NamedTensorMap, outputs: String): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    def executeAsync(inputs: NamedTensorMap, outputs: js.Array[String]): js.Promise[Tensor[Rank] | js.Array[Tensor[Rank]]] = js.native
    
    var executor: js.Any = js.native
    
    var findIOHandler: js.Any = js.native
    
    var handler: js.Any = js.native
    
    val inputNodes: js.Array[String] = js.native
    
    @JSName("inputs")
    val inputs_GraphModel: js.Array[TensorInfo] = js.native
    
    /**
      * Loads the model and weight files, construct the in memory weight map and
      * compile the inference graph.
      */
    def load(): js.Promise[Boolean] = js.native
    
    var loadOptions: js.Any = js.native
    
    /**
      * Synchronously construct the in memory weight map and
      * compile the inference graph.
      */
    /** @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true} */
    def loadSync(artifacts: ModelArtifacts): Boolean = js.native
    
    var modelUrl: js.Any = js.native
    
    val modelVersion: String = js.native
    
    var normalizeInputs: js.Any = js.native
    
    var normalizeOutputs: js.Any = js.native
    
    val outputNodes: js.Array[String] = js.native
    
    @JSName("outputs")
    val outputs_GraphModel: js.Array[TensorInfo] = js.native
    
    def predict(inputs: js.Array[Tensor[Rank]]): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
    /**
      * Execute the inference for the input tensors.
      *
      * @param input The input tensors, when there is single input for the model,
      * inputs param should be a `tf.Tensor`. For models with mutliple inputs,
      * inputs params should be in either `tf.Tensor`[] if the input order is
      * fixed, or otherwise NamedTensorMap format.
      *
      * For model with multiple inputs, we recommend you use NamedTensorMap as the
      * input type, if you use `tf.Tensor`[], the order of the array needs to
      * follow the
      * order of inputNodes array. @see {@link GraphModel.inputNodes}
      *
      * You can also feed any intermediate nodes using the NamedTensorMap as the
      * input type. For example, given the graph
      *    InputNode => Intermediate => OutputNode,
      * you can execute the subgraph Intermediate => OutputNode by calling
      *    model.execute('IntermediateNode' : tf.tensor(...));
      *
      * This is useful for models that uses tf.dynamic_rnn, where the intermediate
      * state needs to be fed manually.
      *
      * For batch inference execution, the tensors for each input need to be
      * concatenated together. For example with mobilenet, the required input shape
      * is [1, 244, 244, 3], which represents the [batch, height, width, channel].
      * If we are provide a batched data of 100 images, the input tensor should be
      * in the shape of [100, 244, 244, 3].
      *
      * @param config Prediction configuration for specifying the batch size and
      * output node names. Currently the batch size option is ignored for graph
      * model.
      *
      * @returns Inference result tensors. The output would be single `tf.Tensor`
      * if model has single output node, otherwise Tensor[] or NamedTensorMap[]
      * will be returned for model with multiple outputs.
      */
    /** @doc {heading: 'Models', subheading: 'Classes'} */
    def predict(inputs: Tensor[Rank]): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
    def predict(inputs: NamedTensorMap): Tensor[Rank] | js.Array[Tensor[Rank]] | NamedTensorMap = js.native
    
    def save(handlerOrURL: String): js.Promise[SaveResult] = js.native
    def save(handlerOrURL: String, config: SaveConfig): js.Promise[SaveResult] = js.native
    /**
      * Save the configuration and/or weights of the GraphModel.
      *
      * An `IOHandler` is an object that has a `save` method of the proper
      * signature defined. The `save` method manages the storing or
      * transmission of serialized data ("artifacts") that represent the
      * model's topology and weights onto or via a specific medium, such as
      * file downloads, local storage, IndexedDB in the web browser and HTTP
      * requests to a server. TensorFlow.js provides `IOHandler`
      * implementations for a number of frequently used saving mediums, such as
      * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`
      * for more details.
      *
      * This method also allows you to refer to certain types of `IOHandler`s
      * as URL-like string shortcuts, such as 'localstorage://' and
      * 'indexeddb://'.
      *
      * Example 1: Save `model`'s topology and weights to browser [local
      * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);
      * then load it back.
      *
      * ```js
      * const modelUrl =
      *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';
      * const model = await tf.loadGraphModel(modelUrl);
      * const zeros = tf.zeros([1, 224, 224, 3]);
      * model.predict(zeros).print();
      *
      * const saveResults = await model.save('localstorage://my-model-1');
      *
      * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');
      * console.log('Prediction from loaded model:');
      * model.predict(zeros).print();
      * ```
      *
      * @param handlerOrURL An instance of `IOHandler` or a URL-like,
      * scheme-based string shortcut for `IOHandler`.
      * @param config Options for saving the model.
      * @returns A `Promise` of `SaveResult`, which summarizes the result of
      * the saving, such as byte sizes of the saved artifacts for the model's
      *   topology and weight values.
      */
    /**
      * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}
      */
    def save(handlerOrURL: IOHandler): js.Promise[SaveResult] = js.native
    def save(handlerOrURL: IOHandler, config: SaveConfig): js.Promise[SaveResult] = js.native
    
    var version: js.Any = js.native
    
    val weights: NamedTensorsMap = js.native
  }
}
