package typingsSlinky.tensorflowTfjsLayers.activationConfigMod

import org.scalablytyped.runtime.TopLevel
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.elu_
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.hard_sigmoid
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.linear
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.relu6
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.relu_
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.selu
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.sigmoid
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.softmax_
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.softplus
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.softsign
import typingsSlinky.tensorflowTfjsLayers.tensorflowTfjsLayersStrings.tanh
import scala.scalajs.js
import scala.scalajs.js.`|`
import scala.scalajs.js.annotation.{JSGlobalScope, JSGlobal, JSImport, JSName, JSBracketAccess}

@JSImport("@tensorflow/tfjs-layers/dist/keras_format/activation_config", "activationOptions")
@js.native
object activationOptions
  extends TopLevel[
      js.Array[
        linear | relu_ | elu_ | relu6 | selu | sigmoid | softmax_ | softplus | tanh | hard_sigmoid | softsign
      ]
    ]
