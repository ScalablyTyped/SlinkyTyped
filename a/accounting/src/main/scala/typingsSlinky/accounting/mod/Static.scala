package typingsSlinky.accounting.mod

import scala.scalajs.js
import scala.scalajs.js.`|`
import scala.scalajs.js.annotation.{JSGlobalScope, JSGlobal, JSImport, JSName, JSBracketAccess}

@js.native
trait Static extends js.Object {
  
  // format a list of values for column-display
  def formatColumn(numbers: js.Array[js.Array[Double] | Double]): js.Array[String] = js.native
  def formatColumn(numbers: js.Array[js.Array[Double] | Double], options: CurrencySettings[CurrencyFormat | String]): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatColumn(numbers: js.Array[js.Array[Double] | Double], symbol: js.UndefOr[scala.Nothing], precision: Double): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: String,
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: String,
    decimal: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: String,
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatColumn(numbers: js.Array[js.Array[Double] | Double], symbol: String): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatColumn(numbers: js.Array[js.Array[Double] | Double], symbol: String, precision: Double): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatColumn(numbers: js.Array[js.Array[Double] | Double], symbol: String, precision: Double, thousand: String): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: Double,
    thousand: String,
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: Double,
    thousand: String,
    decimal: String
  ): js.Array[String] = js.native
  def formatColumn(
    numbers: js.Array[js.Array[Double] | Double],
    symbol: String,
    precision: Double,
    thousand: String,
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  
  // format any number or stringified number into currency
  def formatMoney(
    number: Double | String,
    symbol: js.UndefOr[String],
    precision: js.UndefOr[Double],
    thousand: js.UndefOr[String],
    decimal: js.UndefOr[String],
    format: js.UndefOr[String]
  ): String = js.native
  def formatMoney(number: String, options: CurrencySettings[CurrencyFormat | String]): String = js.native
  def formatMoney(number: Double, options: CurrencySettings[CurrencyFormat | String]): String = js.native
  // generic case (any array of numbers)
  def formatMoney(numbers: js.Array[_ | Double]): js.Array[String] = js.native
  def formatMoney(numbers: js.Array[_ | Double], options: CurrencySettings[CurrencyFormat | String]): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatMoney(numbers: js.Array[_ | Double], symbol: js.UndefOr[scala.Nothing], precision: Double): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: String,
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: String,
    decimal: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: js.UndefOr[scala.Nothing],
    precision: Double,
    thousand: String,
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatMoney(numbers: js.Array[_ | Double], symbol: String): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatMoney(numbers: js.Array[_ | Double], symbol: String, precision: Double): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  def formatMoney(numbers: js.Array[_ | Double], symbol: String, precision: Double, thousand: String): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: Double,
    thousand: String,
    decimal: js.UndefOr[scala.Nothing],
    format: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: Double,
    thousand: String,
    decimal: String
  ): js.Array[String] = js.native
  def formatMoney(
    numbers: js.Array[_ | Double],
    symbol: String,
    precision: Double,
    thousand: String,
    decimal: String,
    format: String
  ): js.Array[String] = js.native
  
  def formatNumber(number: js.Array[_ | Double]): js.Array[String] = js.native
  def formatNumber(number: js.Array[_ | Double], options: NumberSettings): js.Array[String] = js.native
  def formatNumber(
    number: js.Array[_ | Double],
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatNumber(number: js.Array[_ | Double], precision: js.UndefOr[scala.Nothing], thousand: String): js.Array[String] = js.native
  def formatNumber(
    number: js.Array[_ | Double],
    precision: js.UndefOr[scala.Nothing],
    thousand: String,
    decimal: String
  ): js.Array[String] = js.native
  def formatNumber(number: js.Array[_ | Double], precision: Double): js.Array[String] = js.native
  def formatNumber(
    number: js.Array[_ | Double],
    precision: Double,
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): js.Array[String] = js.native
  def formatNumber(number: js.Array[_ | Double], precision: Double, thousand: String): js.Array[String] = js.native
  def formatNumber(number: js.Array[_ | Double], precision: Double, thousand: String, decimal: String): js.Array[String] = js.native
  // format a number with custom precision and localisation
  def formatNumber(number: Double): String = js.native
  def formatNumber(number: Double, options: NumberSettings): String = js.native
  def formatNumber(
    number: Double,
    precision: js.UndefOr[scala.Nothing],
    thousand: js.UndefOr[scala.Nothing],
    decimal: String
  ): String = js.native
  def formatNumber(number: Double, precision: js.UndefOr[scala.Nothing], thousand: String): String = js.native
  def formatNumber(number: Double, precision: js.UndefOr[scala.Nothing], thousand: String, decimal: String): String = js.native
  def formatNumber(number: Double, precision: Double): String = js.native
  def formatNumber(number: Double, precision: Double, thousand: js.UndefOr[scala.Nothing], decimal: String): String = js.native
  def formatNumber(number: Double, precision: Double, thousand: String): String = js.native
  def formatNumber(number: Double, precision: Double, thousand: String, decimal: String): String = js.native
  
  // settings object that controls default parameters for library methods
  var settings: Settings = js.native
  
  // better rounding for floating point numbers
  def toFixed(number: Double): String = js.native
  def toFixed(number: Double, precision: Double): String = js.native
  
  // get a value from any formatted number/currency string
  def unformat(string: String): Double = js.native
  def unformat(string: String, decimal: String): Double = js.native
}
